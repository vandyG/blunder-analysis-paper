\section{Methodology}
\subsection{Data Extraction from Lichess PGN Datasets}
The Lichess PGN archives were converted into an analysis-ready dataset via a two-stage extraction pipeline that prioritizes sequential I/O efficiency and parallelism. In the first stage the corpus is scanned in a single pass to record the byte offsets of individual games; these offsets permit deterministic, non-overlapping assignment of disjoint batches to worker processes so that each portion of the file is read exactly once. In the second stage multiple workers read games by offset, parse headers and move lists into structured records, and extract game-level metadata (for example, time control, termination mode, and Elo ratings). For each ply we collect engine-derived evaluations (centipawn values, win/draw/loss probabilities, and mate distances) through a controlled evaluation interface.

Per-move records are written to a columnar store (Parquet) with a fixed schema so that each row corresponds to a single move event and columns encode numerical and categorical descriptors. This storage choice supports linear-time streaming aggregation and efficient matrix-style batch loading for downstream statistical and machine-learning pipelines. The pipeline design enforces reproducibility (via deterministic offset partitioning), minimizes redundant I/O (single-pass discovery plus parallel readers), and maintains a clear, auditable mapping from raw PGN to analytic features suitable for vectorized analysis.

\subsection{Feature Engineering Using Evaluation Data and Time Metrics}
Let \(x_t \in \mathbb{R}^d\) denote the feature vector extracted for move \(t\). Each per-move record produced by the extractor contains a set of raw fields together with a small number of derived quantities; these form the building blocks for downstream matrices and models. Below we list the record fields, their interpretation, and the calculations used to produce derived features.

\begin{itemize}
    \item \textbf{Identifiers and provenance:} the fields \texttt{game\_id}, \texttt{offset}, and \texttt{site} uniquely identify the game and the byte position within the original archive. These fields enable reproducible joins and deterministic partitioning of the corpus.

    \item \textbf{Player ratings:} the player's Elo values are recorded as integer fields for White and Black. The reported rating changes are stored separately. A simple derived scalar is the rating differential
        \[\Delta\mathrm{Elo} = \texttt{white\_elo} - \texttt{black\_elo}.\]

    \item \textbf{Opening and game descriptors:} \texttt{eco} is the ECO opening code; \texttt{time\_control}, \texttt{game\_time}, and \texttt{increment} encode the time-control category and parameters. The categorical time-control is used to group and standardize temporal features.

    \item \textbf{Game outcome and termination:} \texttt{result} is encoded as +1 (White win), 0 (draw), and -1 (Black win). \texttt{termination} records the termination mode (resignation, mate, timeout, etc.) as a categorical string.

    \item \textbf{Move context:} \texttt{turn} (which side moves), \texttt{move} (SAN move string), and \texttt{fullmove\_number} give the ply context required for sequential analyses.

    \item \textbf{Engine-based evaluations:}
        \begin{itemize}
            \item \texttt{cp\_score}: a centipawn-style score (White-perspective). Mate scores are converted to large numeric sentinels so the field remains numeric for vectorized pipelines.
            \item \texttt{winning\_chance}, \texttt{drawing\_chance}, \texttt{losing\_chance}: probability estimates derived from the engine's WDL output (in \([0,1]\)).
            \item \texttt{mate\_in}: mate distance when applicable; otherwise set to $+\infty$.
            \item \texttt{sharpness}: a scalar derived from WDL probabilities that quantifies how easy the position is to ``mess up'' for either player, defined as $S_t = P_t^{\mathrm{win}} + P_t^{\mathrm{loss}}$ as in Section~\ref{sec:problem_formulation}.
        \end{itemize}

    \item \textbf{Board and tactical context:} \texttt{piece\_moved} is the moved piece type (categorical), \texttt{board\_fen} is the FEN of the position after the ply, and \texttt{is\_check} is a Boolean indicating whether the side to move is in check.

    \item \textbf{Temporal information:} \texttt{clock} records the player's remaining time (seconds) at the moment of the move when available; missing clocks are encoded as null. In addition to raw clocks we compute a normalized \emph{time\_ratio} feature: for each move we look at the previous clock value (including increment) and measure the fraction of available thinking time consumed by the current move, 
    $$\text{time\_ratio}_t = \frac{(\text{clock}_{t-1} + \text{increment}) - \text{clock}_t}{\text{clock}_{t-1} + \text{increment}},$$
    with null ratios set to zero. This ratio absorbs both the base time and increment structure, yielding a comparable scale across formats.

    \item \textbf{Local evaluation change:} the primary short-term derived measure is the per-ply evaluation change
        \[\Delta\mathrm{eval}_t = \texttt{cp\_score}_t - \texttt{cp\_score}_{t-1},\]
        computed as the difference between the engine score after the current ply and the engine score before the same ply. Positive values indicate an advantage shift for White; negative values indicate deterioration. When the previous score is unavailable (e.g., initial position), the change is computed with respect to the starting-position score.
\end{itemize}

From the raw and derived fields we construct analysis-ready features. Typical choices used in this work include:

\begin{itemize}
    \item Intercept: a constant 1 to absorb baseline effects in linear models.
    \item Evaluation features: \texttt{cp\_score} and \(\Delta\mathrm{eval}_t\), optionally winsorized to limit the influence of mate sentinels.
    \item Probabilistic features: \texttt{winning\_chance}, \texttt{drawing\_chance}, and \texttt{losing\_chance} (optionally transformed via log-odds for linear models), together with the sharpness score $S_t = P_t^{\mathrm{win}} + P_t^{\mathrm{loss}}$.
    \item Mate indicators: binary indicator $\mathbf{1}[\texttt{mate\_in} < \infty]$ and the mate distance when present.
    \item Temporal features: standardized remaining clock (within-game z-score), the normalized time-ratio described above, and an indicator or ordinal encoding of time-control category.
    \item Categorical encodings: one-hot or embedding encodings of \texttt{piece\_moved}, \texttt{eco}, and \texttt{termination}.
    \item Rating-based controls: the rating differential $\Delta\mathrm{Elo}$ and raw player ratings.
\end{itemize}

These raw and derived features are persisted in columnar form so that the final design matrix $X \in \mathbb{R}^{n \times d}$ (with rows ordered by move-event) can be loaded without further parsing. 

\subsubsection{Feature Preprocessing and Transformations}
Before applying dimensionality reduction and modeling techniques, several preprocessing transformations are applied to the raw feature set:

\begin{enumerate}
    \item \textbf{Categorical encoding:} Boolean features (\texttt{is\_check}) are cast to integer type (0/1). Categorical features \texttt{time\_control\_type} and \texttt{piece\_type} are one-hot encoded, producing binary indicator columns for each category value. The \texttt{UNKNOWN} category is dropped as it provides no discriminative information.
    
    \item \textbf{Target variable construction:} The response variable \texttt{is\_error} is created as a binary indicator:
    \[
    \texttt{is\_error}_t = \mathbf{1}[\texttt{judgement}_t \in \{\text{Blunder}, \text{Mistake}, \text{Inaccuracy}\}],
    \]
    with null judgements (moves with no evaluation delta) mapped to 0 (no error).
    
    \item \textbf{Evaluation score scaling:} The centipawn score \texttt{cp\_score} exhibits extreme values in mate positions (set to large sentinels such as $\pm10000$). To prevent these outliers from dominating distance-based analyses, we apply a bounded transformation:
    \[
    \texttt{cp\_score}_{\text{scaled}} = 
    \begin{cases}
    \tanh\left(\frac{\texttt{cp\_score}}{2019}\right) \cdot 0.99 & \text{if } \texttt{mate\_in} = 0, \\
    \text{sign}(\texttt{cp\_score}) & \text{if } \texttt{mate\_in} \neq 0.
    \end{cases}
    \]
    The hyperbolic tangent squashes non-mate evaluations into the interval $(-0.99, 0.99)$, preserving order while bounding magnitude. Positions with forced mate are mapped to $\pm1$ to mark them as decisively winning or losing without introducing unbounded variance. The divisor 2019 was chosen empirically to yield smooth compression across typical centipawn ranges.
    
    \item \textbf{Missing value imputation:} Infinite values in \texttt{mate\_in} (indicating no forced mate) are replaced with null, then filled with 0. Similarly, any remaining null values in \texttt{time\_ratio} are set to 0, representing moves with no recorded time consumption.
    
    \item \textbf{Standardization:} Each numeric feature column $f$ (excluding the binary target \texttt{is\_error}) is z-score standardized:
    \[
    f_{\text{std}} = \frac{f - \mu_f}{\sigma_f},
    \]
    where $\mu_f$ and $\sigma_f$ are the mean and standard deviation computed over all moves. This transformation centers each feature at zero and scales to unit variance, ensuring that features with different native ranges (e.g., \texttt{player\_elo} in $[1000, 3000]$ versus \texttt{time\_ratio} in $[0,1]$) contribute comparably to subsequent linear algebraic operations such as PCA.
\end{enumerate}

This preprocessing pipeline produces a standardized, numeric-only feature matrix suitable for least-squares regression, principal component analysis, and classification models, while maintaining clear provenance from the original per-move records to the numeric arrays used in the mathematical analyses.

\subsection{Board Position Encoding: FEN to Bitboard Representation}
\label{sec:fen_encoding}
While the engine evaluation metrics (centipawn scores, WDL probabilities) provide global assessments of position quality, many of the structural and tactical patterns underlying blunders require direct access to the spatial configuration of pieces on the board. The standard representation for chess positions is the Forsyth-Edwards Notation (FEN), a compact string encoding the placement of all pieces, active side, castling rights, en passant targets, halfmove clock, and fullmove number. For example, the starting position is encoded as:
\begin{verbatim}
rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1
\end{verbatim}
where lowercase letters denote black pieces, uppercase letters denote white pieces, digits indicate consecutive empty squares, and slashes separate ranks.

However, FEN strings are not directly amenable to linear algebraic operations or gradient-based learning algorithms. To enable matrix factorization techniques (PCA, SVD) and eventual neural network modeling, we convert each board position into a \emph{bitboard tensor}---a fixed-size binary representation that exposes the spatial structure of the position in a format compatible with standard numerical analysis tools.

\subsubsection{Bitboard Tensor Construction}
Each chess position is encoded as a $12 \times 8 \times 8$ binary tensor, where the first dimension indexes piece type and color:
\[
\mathbf{B} \in \{0,1\}^{12 \times 8 \times 8}.
\]
The 12 channels correspond to the six piece types (pawn, knight, bishop, rook, queen, king) for each color (white, black), yielding planes:
\[
\{\text{WP, WN, WB, WR, WQ, WK, BP, BN, BB, BR, BQ, BK}\}.
\]
For each plane $p$ and square $(i,j)$ with $i,j \in \{0,\ldots,7\}$ (representing rank and file), the entry $\mathbf{B}_{p,i,j}$ is set to 1 if the corresponding piece occupies that square, and 0 otherwise. This representation is standard in modern chess engines and neural network architectures (e.g., AlphaZero~\cite{silver2018general}).

\subsubsection{Flattening for Linear Algebra}
For compatibility with dimensionality reduction techniques such as PCA and SVD, the $12 \times 8 \times 8$ tensor is flattened into a single 768-dimensional binary vector:
\[
\mathbf{b} = \text{vec}(\mathbf{B}) \in \{0,1\}^{768}.
\]
This flattening operation concatenates all 12 planes in a fixed order, producing a canonical vectorized representation of the position. Each move in the dataset is thus associated with a feature vector $\mathbf{x}_t \in \mathbb{R}^{d}$ that combines:
\begin{itemize}
    \item the 768-dimensional bitboard encoding $\mathbf{b}_t$,
    \item scalar evaluation and temporal features (centipawn score, clock, time ratio, sharpness),
    \item categorical encodings (piece moved, opening, time control).
\end{itemize}
The full design matrix $X \in \mathbb{R}^{n \times d}$ therefore incorporates both positional structure (via bitboards) and contextual metadata (via derived scalars), enabling joint analysis of spatial patterns and external factors.

Converting FEN strings to bitboard tensors transforms a symbolic, human-readable representation into a numerical format suited for rigorous mathematical analysis. This encoding bridges classical statistical methods (multivariate regression, PCA) and modern machine learning techniques (neural networks, transformers), forming the foundation for all subsequent modeling and interpretation tasks in this research.

\textbf{Note on dimensionality constraints:} The 768-dimensional bitboard encoding $\mathbf{b}_t$ was excluded from the initial PCA experiments reported in Section~\ref{sec:results} due to memory limitations. The full dataset (approximately $10^8$ move records) combined with high-dimensional spatial features exceeds available RAM for batch matrix operations. Future work will address this constraint through: (i) streaming PCA implementations that process mini-batches sequentially without materializing the full covariance matrix~\cite{IncrementalPCA2019}, (ii) sparse representations that exploit the typical occupancy of $\approx32$ squares per position, or (iii) convolutional autoencoders pretrained on position embeddings to reduce the spatial feature dimension before joint PCA. The analyses in this paper focus on the lower-dimensional evaluation, temporal, and categorical features, which remain highly informative for blunder prediction while fitting comfortably in memory.

\subsection{Dimensionality Reduction via Incremental PCA}
\label{sec:ipca}

Principal Component Analysis (PCA) is a fundamental tool for identifying directions of maximum variance in high-dimensional data and for reducing feature dimensionality while retaining explanatory power. Standard batch PCA requires constructing the full covariance matrix $C = \frac{1}{n}X^T X$ where $X \in \mathbb{R}^{n \times d}$ is the centered data matrix, then computing its eigendecomposition. For our datasets, this computation exceeds available memory.

To address this constraint, we employ \emph{Incremental PCA} (IPCA)~\cite{IncrementalPCA2019}, an online algorithm that processes the data in sequential mini-batches and updates the principal component estimates incrementally. The IPCA algorithm maintains a running estimate of the covariance matrix and its eigendecomposition without requiring the entire dataset to reside in memory simultaneously. Formally, IPCA is equivalent to batch PCA in the sense that given the same dataset, the final transformation coefficients are identical; the difference lies solely in the computational strategy.

\subsubsection{Implementation Details}
In our experiments we selected $k = 10$ principal components and processed batches of 10,000 move records at a time. The features included in the analysis comprised:
\begin{itemize}
    \item Temporal features: \texttt{game\_time}, \texttt{increment}, \texttt{time\_ratio}.
    \item Evaluation features: scaled \texttt{cp\_score}, \texttt{winning\_chance}, \texttt{drawing\_chance}, \texttt{losing\_chance}, \texttt{eval\_delta}.
    \item Positional context: \texttt{turn}, \texttt{is\_check}, \texttt{mate\_in}.
    \item Player skill: \texttt{player\_elo}.
    \item One-hot encoded categories: time control types (Blitz, Bullet, Rapid, Standard, Unlimited) and piece types (Pawn, Knight, Bishop, Rook, Queen, King).
\end{itemize}
After preprocessing, standardization, and one-hot expansion, the feature dimension was $d = 23$.

The target variable \texttt{is\_error} was excluded from the PCA fitting but retained alongside the projected features for subsequent supervised modeling. This separation ensures that the principal components reflect the intrinsic structure of the input features without contamination from the response variable.

The use of IPCA enables scalable analysis of datasets that far exceed available memory, making it feasible to extract low-dimensional representations from massive corpora while maintaining mathematical equivalence to classical batch PCA. The resulting principal components serve as input features for downstream regression and classification models aimed at blunder prediction.
